# -*- coding: utf-8 -*-
"""
Created on Tue Aug  4 16:20:29 2020

@author: funrr
"""


import argparse
import numpy as np
import os
import pickle
import argparse
from collections import namedtuple

# os.environ['KMP_DUPLICATE_LIB_OK']='True'
# import matplotlib.pyplot as plt
from scipy.spatial import distance
from utils import euclidian, kmeans, two_opt 




depot = [0.5, 0.5]
CAPACITY = 1000

#TODO: solution may have some problem

'''
generate training data
'''



class VrptwData():
    
    '''data generator for cvrptw:
        B: batch sizedata_gern
        size: problem size
        num_vehicle:  the problem need at least how many vehicles of capacity 1000 to satisfy
        one_veh: set up the problem as one vehicle with multiple returns or a fleet of vehicles which only return once
        TW_RELAX_FACTOR: relax the problem by larger time windows (larger means more relax)
        CAPACITY_RELAX_FACTOR: relax the problem by smaller demands (smaller means more relax)
        #############################
        X: [batch_size, problem, size, 5] where depot is the first node in a problem
        for each node, it is[x_cor, y_cor, time_visit, time_leave, demand]
        solution: a solution generated by 2-Opt
        all_routes: a sequence of nodes visited by 2-OPT   
        '''
    
    def __init__(self, args, time_factor=100.0, one_veh = None, TW_RELAX_FACTOR = None, CAPACITY_RELAX_FACTOR = None):
        
        

        self.B = int(args['batch_size'])
        self.size  = int(args['size'])
        self.num_vehicle  = int(args['num_vehicle'])
        self.one_veh = one_veh
        if TW_RELAX_FACTOR is not None:
            self.TW_RELAX_FACTOR = TW_RELAX_FACTOR
        else:
            self.TW_RELAX_FACTOR = args['TW_RELAX_FACTOR']
        if CAPACITY_RELAX_FACTOR is not None:
            self.CAPACITY_RELAX_FACTOR = CAPACITY_RELAX_FACTOR
        else:
            self.CAPACITY_RELAX_FACTOR = args['CAPACITY_RELAX_FACTOR']

    
    def get_batch(self):
        return self.generate_data()
    
    def save_data(self, path, sample = None, seed = None):
        X, solution, all_routes = self.generate_data(seed)
        
        if sample is not None:
            X_name = 'cvrptw-size-'+str(self.B)+'-len-'+str(self.size)+'('+str(sample)+')'
            X_path = os.path.join(path, X_name)
            sol_name = '2OPT_Solution-' + X_name
            sol_path = os.path.join(path, sol_name)
            route_name = '2OPT_Routing-' + X_name
            route_path = os.path.join(path, route_name)

        np.save(X_path, X)
        np.save(sol_path, solution)
        np.save(route_path, all_routes)
        
    def load_data(self, path):
        return np.load(path)
        
    def generate_data(self, seed = None):
        B=self.B 
        size=self.size
        num_vehicle = self.num_vehicle 
        one_veh = self.one_veh
        TW_RELAX_FACTOR = self.TW_RELAX_FACTOR,
        CAPACITY_RELAX_FACTOR = self.CAPACITY_RELAX_FACTOR
        
        if seed is not None:
            np.random.seed(seed)
            print("SEED", seed)
        graph = np.random.rand(size, B, 2)
        #graph_with_depot = np.insert(graph, 0 , depot, axis = 0)  #insert depot to the center in each problem
        solutions = np.zeros(B) 
        X = np.zeros([size+1, B, 5])  #init data( xi, yi, ei, li, ci )  (51, 512, 5)
        #solutions = np.zeros(B)  
        X[0, :, :2] = depot #set depot loc
        X[0, :, 3] = 1000  #arbitrary large depot leave time 
        all_routes = np.zeros([size+1+5, B, 5])
        all_routes[0,:,:] = X[0, :, :]
    ##......
        for problem in range(B):
            cluster_dataset = graph[:,problem,:]   #every node without depot
            #print(cluster_dataset)
    
            centroid, _, belong_to = kmeans(cluster_dataset, num_vehicle, epsilon=0, distance='euclidian')  ##??
            
            while np.unique(belong_to).size < num_vehicle:
                cluster_dataset = np.random.rand(size, 2)
                centroid, _, belong_to = kmeans(cluster_dataset, num_vehicle, epsilon=0, distance='euclidian')
                
            #group nodes in to vector based on cluster (list of num_vehicle)
            route_coors = []
            best_routes = []
            routes_dmatrix = []
    
            for route_idx in range(num_vehicle):  
                #print('route', route_idx)
                route_coor = [graph[i, problem, :] for i in  range(size) if belong_to[i] == route_idx]
                route_coor = np.insert(np.vstack(route_coor), 0 , depot, axis = 0)  #coor of each route(include depot), eg (11,2)
                #print(route_coor.size)
    
    
    
                route_coors.append(route_coor)  #k routes with depot in each route

                ################finish build routes, begin to 2 opt each     
                dmatrix = distance.cdist(route_coor, route_coor, 'euclidean')   #distance matrix of the current route eg(11,11)
                routes_dmatrix.append(dmatrix)
                
                best = two_opt(dmatrix)
                best_routes.append(best)    

                #now got route_coors(all nodes) and best_routes(indexs)
                        ##########build time vec for problem b#################
            
            ############# build X ############
            k = 1    #node counter
            l = 1   #counter for all_route
            if one_veh: #set up routes visiting sequence based on centorid 
                centroid_dmatrix =  distance.cdist(centroid, centroid, 'euclidean')
                centroid_best = two_opt(centroid_dmatrix)[:-1]
                route_visit_seq = centroid_best
                cur_time = 0  
            else:
                route_time = []
                route_visit_seq = np.arange(num_vehicle)
    
            for route in route_visit_seq:
                cur_route = best_routes[route].copy()              
                dmatrix = routes_dmatrix[route]
                demands = np.random.randint(25, 75, len(cur_route)-2)
                demands = demands/demands.sum()*CAPACITY_RELAX_FACTOR
                back_to_depot = False   #every route back to depot once
                if not one_veh:
                    cur_time = 0 
                #############################################################demands = CAPACITY/(len(cur_route)-2)
                for idx, node in enumerate(cur_route):  #index 
                    
                ################# 
                    if node == 0:
                        if  back_to_depot:  #if is back to depot, add extra time going back to depot
                            cur_time += dmatrix[cur_route[idx-1], 0]       
                            all_routes[l, problem, :] = [.5, .5, 0, 1000, 0]
                            l = l+1
                        else:
                            back_to_depot = True    #if is the first time in depot, then the next time is back to depot
                        continue
                    ################
                    ###coor
                    X[k, problem, :2] = route_coors[route][node]  #plug the node coordinate into generated data
                    ###time window
                    cur_time += dmatrix[cur_route[idx-1], cur_route[idx]]
                    #print(cur_time)
                    #tour_len += dmatrix[cur_route[idx-1], cur_route[idx]]
                    X[k, problem, 2] = np.max([0, cur_time - TW_RELAX_FACTOR*np.random.uniform(low=0.4, high=1.0,size = 1)])  # entering time 0<= ei <= cur_time
                    X[k, problem, 3] = cur_time + TW_RELAX_FACTOR*np.random.uniform(low=0.4, high=1.0,size = 1) + 1  # leaving time li >= cur_time
                    X[k, problem, 4] = demands[idx-1]  #feed randomly generated demand to the node
                    all_routes[l, problem, :] = X[k, problem, :]
    
                    #.....
                    k = k+1
                    l = l+1
                if not one_veh:            
                    route_time.append(cur_time)
                    
            #solution upperbound       
            if one_veh:        
                solutions[problem] = cur_time
            else:    
                # print(route_time)
                # print(sum(route_time))
                
                # print(solutions[problem])
                solutions[problem] = sum(route_time)
            
    
            # for idx in range(all_routes.shape[0]):
            #     plt.pause(0.1)
            #     plt.plot(all_routes[:idx+1, problem, 0], all_routes[:idx+1, problem, 1])
    
            
        
        ##### shuffle all nodes index except the depot
        depots = X[0, :,:]  
        nodes = X[1:, :,:]
        np.random.shuffle(nodes)    
        X = np.insert(nodes, 0 , depots, axis = 0)  #size, B, 5
        X = X.transpose(1,0,2)  #B, size, 5
        
        
        return X, solutions, all_routes.transpose(1,0,2)
        #return X, solutions
    
class VrptwData_simple():
    '''data generator for cvrptw:
        B: batch sizedata_gern
        size: problem size
    not stable, need to adjust sacle
        '''
    
    def __init__(self, args, normalize = False):
        
        self.B = int(args['batch_size'])
        self.num_cust  = int(args['size'])
        self.depot_loc  = np.array([[0.5, 0.5]])
        self.demand_mu = 15
        self.demand_sigma = 10
        self.depot_tw = [0, 1000]
        self.normalize =  normalize
        #self.capacity = 1000

    def normalize_to_one(data):
        for i in range(5):
            data[:,:,i] = data[:,:,i]/data[:,:,i].max()
        
    def get_batch(self):
        return self.generate_data()
    
    def demand_sample(self):
        #sample demand from Normal_dist
        demand = np.random.normal(self.demand_mu, self.demand_sigma, self.num_cust)
        #clamp to [1,42]
       
        demand= np.floor(np.absolute(demand))
        demand = np.array([max(item, 1) for item in demand])
        demand = np.array([min(item, 42) for item in demand])
        
        
        demand = demand/42/5
        
        
        return np.expand_dims(demand, axis=1)
    
    #TODO: sample location using cluster?
    def location_sample(self):
        #sample node location uniformly
        if self.normalize:
            location = np.random.uniform(0, 1, (self.num_cust,2))
        else:
            location = np.random.uniform(0, 100, (self.num_cust,2))
        return location
    
    def tw_sample(self, location, depot_loc):
        if self.normalize:
            b0 = 10
        else: 
            b0 = self.depot_tw[1]
        hi = np.ceil(np.linalg.norm(location - depot_loc, axis = 1))  + 1 #hi is distances between depot and custom + 1 (#cust,)
        sample_horizon = np.append(hi.reshape([-1, 1]), (b0 - hi).reshape([-1, 1]), axis = 1)  # sample interval for each cust(#cust, 2)
        start_time = np.floor(np.random.uniform(sample_horizon[:,0], sample_horizon[:,1], (self.num_cust,1)))
        
        eps = np.maximum(abs(np.random.randn(self.num_cust,)),1/100)
        end_time = np.minimum(np.floor(eps*300 + start_time.reshape(-1,)), sample_horizon[:,1])
        return np.append(start_time, end_time.reshape(-1,1), axis=1)
    
            
    def generate_data(self, seed = None):
        B=self.B 
        size=self.num_cust
        
        X =  np.zeros((B, size+1, 5))
        depot = np.array([[0.5,0.5,0,1000, 0]])
        if seed is not None:
            np.random.seed(seed)
            print("SEED", seed)
        for problem in range(B):
            demand = self.demand_sample()
            loc = self.location_sample()
            tw = self.tw_sample(loc, self.depot_loc)
            p = np.concatenate((loc, tw, demand), axis = 1)
            p = np.insert(p, 0,  depot, axis = 0)
            X[problem] = p
        
        
        return X

class JAMPR_data():
    
    def __init__(self, args, normalize = False):    
        self.B = int(args['batch_size'])
        self.num_cust  = int(args['size'])
    
        self.depot_tw = [0, 1000]
        self.normalize =  normalize
        self.time_factor = int(args['time_factor'])
        
    def transform_data_structure(self, raw_data):
        B = self.B
        size = self.num_cust
        X = np.zeros((B, size+1, 5))
        for idx, ins in enumerate(raw_data):
            X[idx, 0, :2] = ins[0]
            X[idx, 1:, :2] = ins[1]
            X[idx, 0, 4] = 0
            X[idx, 1:, 4] = ins[2]
            X[idx, 0, 2:4] = [0, 1000]
            X[idx, 1:, 2:4] = ins[5]
            
            
        return X

    def generate_data(self, rnds=None,
                         service_window=1000,
                         service_duration=0,
                         tw_expansion=3.0):
        """Generate data for CVRP-TW
    
        Args:
            size (int): size of dataset
            graph_size (int): size of problem instance graph (number of customers without depot)
            rnds : numpy random state
            service_window (int): maximum of time units
            service_duration (int): duration of service
            time_factor (float): value to map from distances in [0, 1] to time units (transit times)
            tw_expansion (float): expansion factor of TW w.r.t. service duration
    
        Returns:
            List of CVRP-TW instances wrapped in named tuples
        """
        size = self.B
        graph_size = self.num_cust
        time_factor = self.time_factor
        TW_CAPACITIES = {
        10: 250.,
        20: 500.,
        30: 600.,
        40: 700.,
        50: 750.,
        100: 1000.
    }



        CVRPTW_SET = namedtuple("CVRPTW_SET",
                                ["depot_loc",    # Depot location
                                 "node_loc",     # Node locations
                                 "demand",       # demand per node
                                 "capacity",     # vehicle capacity (homogeneous)
                                 "depot_tw",     # depot time window (full horizon)
                                 "node_tw",      # node time windows
                                 "durations",    # service duration per node
                                 "service_window",  # maximum of time units
                                 "time_factor"])    # value to map from distances in [0, 1] to time units (transit times)
        

        
        rnds = np.random if rnds is None else rnds
    
        ## sample locations
        ##Wei: [0,1] coordinate
        dloc = rnds.uniform(size=(size, 2))  # depot location
        nloc = rnds.uniform(size=(size, graph_size, 2))  # node locations
    
        # TW start needs to be feasibly reachable directly from depot
        
        ##Wei: 100*location to calculate the distance 
        min_t = np.ceil(np.linalg.norm(dloc[:, None, :]*time_factor - nloc*time_factor, axis=-1)) + 1
        # TW end needs to be early enough to perform service and return to depot until end of service window
        max_t = np.ceil(np.linalg.norm(dloc[:, None, :]*time_factor - nloc*time_factor, axis=-1) + service_duration) + 1
    
        # horizon allows for the feasibility of reaching nodes / returning from nodes within the global tw (service window)
        horizon = list(zip(min_t, service_window - max_t))
        epsilon = np.maximum(np.abs(rnds.standard_normal([size, graph_size])), 1 / time_factor)
    
        # sample earliest start times a
        
        ##wei: start time sample from the horizon [batchsize* problem size]
        
        a = [rnds.randint(*h) for h in horizon]
        # calculate latest start times b, which is
        # a + service_time_expansion x normal random noise, all limited by the horizon
        # and combine it with a to create the time windows
        
        
        tw = [np.transpose(np.vstack((rt,  # a
                                      np.minimum(rt + tw_expansion * time_factor * sd, h[-1]).astype(int)  # b
                                      ))).tolist()
              for rt, sd, h in zip(a, epsilon, horizon)]
        
        X = self.transform_data_structure([CVRPTW_SET(*data) for data in zip(
            dloc.tolist(),    #depot loc
            nloc.tolist(),    #node loc
            np.minimum(np.maximum(np.abs(rnds.normal(loc=15, scale=10, size=[size, graph_size])).astype(int), 1), 42).tolist(), #demand???
            np.full(size, TW_CAPACITIES[graph_size]).tolist(),  #capacity
            [[0, service_window]] * size,   #total horizon
            tw,
            np.full([size, graph_size], service_duration).tolist(), #service duration
            [service_window] * size,
            [time_factor] * size,
        )])
    
        return X
